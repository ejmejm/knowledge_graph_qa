{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GraphConv, TopKPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ls11-www.cs.tu-dortmund.de/people/morris/graphkerneldatasets/PROTEINS.zip\n",
      "Extracting data/PROTEINS/PROTEINS/PROTEINS.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "path = osp.join('data', 'PROTEINS')\n",
    "dataset = TUDataset(path, name='PROTEINS')\n",
    "dataset = dataset.shuffle()\n",
    "n = len(dataset) // 10\n",
    "test_dataset = dataset[:n]\n",
    "train_dataset = dataset[n:]\n",
    "test_loader = DataLoader(test_dataset, batch_size=60)\n",
    "train_loader = DataLoader(train_dataset, batch_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = GraphConv(dataset.num_features, 128)\n",
    "        self.pool1 = TopKPooling(128, ratio=0.8)\n",
    "        self.conv2 = GraphConv(128, 128)\n",
    "        self.pool2 = TopKPooling(128, ratio=0.8)\n",
    "        self.conv3 = GraphConv(128, 128)\n",
    "        self.pool3 = TopKPooling(128, ratio=0.8)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(256, 128)\n",
    "        self.lin2 = torch.nn.Linear(128, 64)\n",
    "        self.lin3 = torch.nn.Linear(64, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        print(x, edge_index, batch)\n",
    "        print(self.conv1(x, edge_index))\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool3(x, edge_index, None, batch)\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = x1 + x2 + x3\n",
    "\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.log_softmax(self.lin3(x), dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, data.y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data).max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-047c9b6e4d78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mloss_all\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_graphs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_all' is not defined"
     ]
    }
   ],
   "source": [
    "model.train\n",
    "for data in train_loader:\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, data.y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.64992, Train Acc: 0.71956, Test Acc: 0.71171\n",
      "Epoch: 002, Loss: 0.58320, Train Acc: 0.72156, Test Acc: 0.72973\n",
      "Epoch: 003, Loss: 0.59073, Train Acc: 0.73553, Test Acc: 0.76577\n",
      "Epoch: 004, Loss: 0.56426, Train Acc: 0.74052, Test Acc: 0.73874\n",
      "Epoch: 005, Loss: 0.55536, Train Acc: 0.74750, Test Acc: 0.78378\n",
      "Epoch: 006, Loss: 0.54242, Train Acc: 0.75948, Test Acc: 0.78378\n",
      "Epoch: 007, Loss: 0.53594, Train Acc: 0.75649, Test Acc: 0.78378\n",
      "Epoch: 008, Loss: 0.53390, Train Acc: 0.75749, Test Acc: 0.78378\n",
      "Epoch: 009, Loss: 0.53351, Train Acc: 0.75649, Test Acc: 0.77477\n",
      "Epoch: 010, Loss: 0.53321, Train Acc: 0.75749, Test Acc: 0.74775\n",
      "Epoch: 011, Loss: 0.53031, Train Acc: 0.76248, Test Acc: 0.77477\n",
      "Epoch: 012, Loss: 0.53097, Train Acc: 0.75848, Test Acc: 0.79279\n",
      "Epoch: 013, Loss: 0.52178, Train Acc: 0.76347, Test Acc: 0.79279\n",
      "Epoch: 014, Loss: 0.51979, Train Acc: 0.76747, Test Acc: 0.75676\n",
      "Epoch: 015, Loss: 0.51881, Train Acc: 0.76647, Test Acc: 0.74775\n",
      "Epoch: 016, Loss: 0.50889, Train Acc: 0.76547, Test Acc: 0.78378\n",
      "Epoch: 017, Loss: 0.51419, Train Acc: 0.76248, Test Acc: 0.80180\n",
      "Epoch: 018, Loss: 0.51868, Train Acc: 0.77246, Test Acc: 0.77477\n",
      "Epoch: 019, Loss: 0.50322, Train Acc: 0.76946, Test Acc: 0.76577\n",
      "Epoch: 020, Loss: 0.50965, Train Acc: 0.77445, Test Acc: 0.74775\n",
      "Epoch: 021, Loss: 0.50537, Train Acc: 0.76547, Test Acc: 0.76577\n",
      "Epoch: 022, Loss: 0.51668, Train Acc: 0.76647, Test Acc: 0.78378\n",
      "Epoch: 023, Loss: 0.50028, Train Acc: 0.77046, Test Acc: 0.75676\n",
      "Epoch: 024, Loss: 0.50159, Train Acc: 0.77844, Test Acc: 0.78378\n",
      "Epoch: 025, Loss: 0.50639, Train Acc: 0.77046, Test Acc: 0.77477\n",
      "Epoch: 026, Loss: 0.50617, Train Acc: 0.77445, Test Acc: 0.74775\n",
      "Epoch: 027, Loss: 0.49560, Train Acc: 0.77445, Test Acc: 0.77477\n",
      "Epoch: 028, Loss: 0.50431, Train Acc: 0.77345, Test Acc: 0.79279\n",
      "Epoch: 029, Loss: 0.49921, Train Acc: 0.77545, Test Acc: 0.80180\n",
      "Epoch: 030, Loss: 0.50582, Train Acc: 0.77345, Test Acc: 0.79279\n",
      "Epoch: 031, Loss: 0.49235, Train Acc: 0.77445, Test Acc: 0.76577\n",
      "Epoch: 032, Loss: 0.49885, Train Acc: 0.77445, Test Acc: 0.76577\n",
      "Epoch: 033, Loss: 0.48400, Train Acc: 0.77944, Test Acc: 0.77477\n",
      "Epoch: 034, Loss: 0.48614, Train Acc: 0.78543, Test Acc: 0.78378\n",
      "Epoch: 035, Loss: 0.47784, Train Acc: 0.78443, Test Acc: 0.78378\n",
      "Epoch: 036, Loss: 0.48004, Train Acc: 0.78443, Test Acc: 0.78378\n",
      "Epoch: 037, Loss: 0.46993, Train Acc: 0.77745, Test Acc: 0.78378\n",
      "Epoch: 038, Loss: 0.47689, Train Acc: 0.79142, Test Acc: 0.78378\n",
      "Epoch: 039, Loss: 0.47172, Train Acc: 0.77445, Test Acc: 0.77477\n",
      "Epoch: 040, Loss: 0.47778, Train Acc: 0.78244, Test Acc: 0.78378\n",
      "Epoch: 041, Loss: 0.46949, Train Acc: 0.79142, Test Acc: 0.76577\n",
      "Epoch: 042, Loss: 0.46798, Train Acc: 0.78842, Test Acc: 0.76577\n",
      "Epoch: 043, Loss: 0.46617, Train Acc: 0.79042, Test Acc: 0.76577\n",
      "Epoch: 044, Loss: 0.46337, Train Acc: 0.79441, Test Acc: 0.80180\n",
      "Epoch: 045, Loss: 0.46259, Train Acc: 0.79541, Test Acc: 0.81081\n",
      "Epoch: 046, Loss: 0.45757, Train Acc: 0.79242, Test Acc: 0.76577\n",
      "Epoch: 047, Loss: 0.46504, Train Acc: 0.78942, Test Acc: 0.77477\n",
      "Epoch: 048, Loss: 0.45798, Train Acc: 0.78842, Test Acc: 0.77477\n",
      "Epoch: 049, Loss: 0.44830, Train Acc: 0.79042, Test Acc: 0.76577\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    loss = train(epoch)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print('Epoch: {:03d}, Loss: {:.5f}, Train Acc: {:.5f}, Test Acc: {:.5f}'.\n",
    "          format(epoch, loss, train_acc, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
